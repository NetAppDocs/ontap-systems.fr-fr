---
permalink: asa400/chassis-replace-shutdown.html 
sidebar: sidebar 
keywords: asa a400, asa, a400, component, system, function, properly, contact, technical, support, replace, chassis, shut, down, controller, replacing, remove, module, fan, equipment, rack, cabinet, ha, state, switch, back, aggregate, two-node, metrocluster, configuration, complete, replacement, process, replace the chassis, shut down the controllers when replacing a chassis, remove the controller modules, move the fans, replace a chassis from within the equipment rack or system cabinet, install the controller modules, verify and set the ha state of the chassis, switch back aggregates in a two-node metrocluster configuration, complete the replacement process 
summary: Pour remplacer le châssis, vous devez arrêter les contrôleurs. 
---
= Arrêter les contrôleurs - ASA A400
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


[role="lead"]
Arrêtez ou prenez le contrôleur défaillant en suivant la procédure appropriée pour votre configuration.



== Option 1 : arrêter les contrôleurs lors du remplacement d'un châssis

Cette procédure concerne uniquement les configurations à 2 nœuds non MetroCluster. Si votre système comporte plus de deux nœuds, reportez-vous à la section https://kb.netapp.com/Advice_and_Troubleshooting/Data_Storage_Software/ONTAP_OS/How_to_perform_a_graceful_shutdown_and_power_up_of_one_HA_pair_in_a_4__node_cluster["Comment effectuer un arrêt normal et mettre sous tension une paire haute disponibilité dans un cluster à 4 nœuds"^].

.Avant de commencer
Éléments requis :

* Informations d'identification de l'administrateur local pour ONTAP.
* Phrase secrète pour la gestion des clés intégrée NetApp à l'échelle du cluster (OKM) en cas d'utilisation du chiffrement du stockage.
* Accès SP/BMC pour chaque contrôleur.
* Empêchez tous les clients/hôtes d'accéder aux données sur le système NetApp.
* Suspendre les tâches de sauvegarde externes.
* Outils et équipements nécessaires au remplacement.



NOTE: Si le système est un système NetApp StorageGRID ou ONTAP S3 utilisé en tant que Tier cloud FabricPool, reportez-vous au https://kb.netapp.com/onprem/ontap/hardware/What_is_the_procedure_for_graceful_shutdown_and_power_up_of_a_storage_system_during_scheduled_power_outage#["Arrêtez et mettez votre système de stockage sous tension en toute simplicité"] après avoir effectué cette procédure.


NOTE: Si vous utilisez des LUN de baies FlexArray, suivez la documentation spécifique du fournisseur à la baie de stockage pour la procédure d'arrêt à effectuer pour ces systèmes après avoir réalisé cette procédure.


NOTE: Si vous utilisez des disques SSD, reportez-vous à la section https://kb.netapp.com/Support_Bulletins/Customer_Bulletins/SU490["SU490 : meilleures pratiques relatives aux SSD (impact : critique) : évitez les risques de panne disque et de perte de données si vous les mettez hors tension pendant plus de deux mois"]

Avant de procéder à l'arrêt, vous devez :

* Effectuer des opérations supplémentaires https://kb.netapp.com/onprem/ontap/os/How_to_perform_a_cluster_health_check_with_a_script_in_ONTAP["vérifications de l'état du système"].
* Mettez à niveau ONTAP vers une version recommandée pour le système.
* Résoudre tout https://activeiq.netapp.com/["Alertes et risques liés au bien-être Active IQ"]. Notez toutes les anomalies présentes sur le système, telles que les voyants des composants du système.


.Étapes
. Connectez-vous au cluster via SSH ou connectez-vous à un nœud du cluster à l'aide d'un câble de console local et d'un ordinateur portable/d'une console.
. Désactivez AutoSupport et indiquez la durée pendant laquelle vous vous attendez à ce que le système soit hors ligne :
+
`system node autosupport invoke -node * -type all -message "MAINT=8h Power Maintenance"`

. Identifiez l'adresse SP/BMC de tous les nœuds :
+
`system service-processor show -node * -fields address`

. Quitter le cluster shell : `exit`
. Connectez-vous au processeur de service/contrôleur BMC via SSH en utilisant l'adresse IP de l'un des nœuds répertoriés dans le résultat de l'étape précédente.
+
Si votre utilise une console ou un ordinateur portable, connectez-vous au contrôleur à l'aide des mêmes informations d'identification d'administrateur de cluster.

+

NOTE: Ouvrez une session SSH sur chaque connexion SP/BMC afin de pouvoir surveiller la progression.

. Arrêter tous les nœuds du cluster :
+
`system node halt -node * -skip-lif-migration-before-shutdown true -ignore-quorum-warnings true -inhibit-takeover true`.

+

NOTE: Pour les clusters qui utilisent SnapMirror en mode synchrone : `system node halt -node * -skip-lif-migration-before-shutdown true -ignore-quorum-warnings true -inhibit-takeover true -ignore-strict-sync-warnings true`

. Entrez *y* pour chaque contrôleur du cluster lorsque vous voyez `_Warning: Are you sure you want to halt node "cluster name-controller number"?
{y|n}:_`
. Attendez que chaque contrôleur s'arrête et affichez l'invite DU CHARGEUR.
. Mettez chaque bloc d'alimentation hors tension ou débranchez-les s'il n'y a pas d'interrupteur marche/arrêt du bloc d'alimentation.
. Débranchez le cordon d'alimentation de chaque bloc d'alimentation.
. Vérifiez que tous les contrôleurs du châssis défectueux sont hors tension.




== Option 2 : arrêter un contrôleur dans une configuration MetroCluster à deux nœuds

Pour arrêter le contrôleur défaillant, vous devez déterminer l'état du contrôleur et, si nécessaire, basculer le contrôleur de sorte que ce dernier continue de transmettre des données depuis le stockage défaillant du contrôleur.

.Description de la tâche
* Si vous utilisez NetApp Storage Encryption, vous devez avoir réinitialisé le MSID à l'aide des instructions de la section « Return a FIPS drive or SED to Unprotected mode » de link:https://docs.netapp.com/us-en/ontap/encryption-at-rest/return-seds-unprotected-mode-task.html["Présentation du chiffrement NetApp avec l'interface de ligne de commande"^].
* Vous devez laisser les alimentations allumées à l'issue de cette procédure pour fournir une alimentation au contrôleur en état.


.Étapes
. Vérifiez l'état du contrôleur MetroCluster pour déterminer si le contrôleur défectueux a automatiquement basculé sur le contrôleur en bon état : `metrocluster show`
. Selon qu'un basculement automatique s'est produit, suivre le tableau suivant :
+
[cols="1,2"]
|===
| En cas de dysfonctionnement du contrôleur... | Alors... 


 a| 
A automatiquement basculé
 a| 
Passez à l'étape suivante.



 a| 
N'a pas été automatiquement commutée
 a| 
Effectuer un basculement planifié à partir du contrôleur en bon état : `metrocluster switchover`



 a| 
N'a pas été automatiquement commutée, vous avez tenté de basculer avec le `metrocluster switchover` la commande, et le basculement a été vetoté
 a| 
Examinez les messages de veto et, si possible, résolvez le problème et réessayez. Si vous ne parvenez pas à résoudre le problème, contactez le support technique.

|===
. Resynchroniser les agrégats de données en exécutant le `metrocluster heal -phase aggregates` commande provenant du cluster survivant.
+
[listing]
----
controller_A_1::> metrocluster heal -phase aggregates
[Job 130] Job succeeded: Heal Aggregates is successful.
----
+
Si la guérison est vetotée, vous avez la possibilité de réémettre le `metrocluster heal` commande avec `-override-vetoes` paramètre. Si vous utilisez ce paramètre facultatif, le système remplace tout veto logiciel qui empêche l'opération de correction.

. Vérifiez que l'opération a été terminée à l'aide de la commande MetroCluster Operation show.
+
[listing]
----
controller_A_1::> metrocluster operation show
    Operation: heal-aggregates
      State: successful
Start Time: 7/25/2016 18:45:55
   End Time: 7/25/2016 18:45:56
     Errors: -
----
. Vérifier l'état des agrégats à l'aide de `storage aggregate show` commande.
+
[listing]
----
controller_A_1::> storage aggregate show
Aggregate     Size Available Used% State   #Vols  Nodes            RAID Status
--------- -------- --------- ----- ------- ------ ---------------- ------------
...
aggr_b2    227.1GB   227.1GB    0% online       0 mcc1-a2          raid_dp, mirrored, normal...
----
. Réparez les agrégats racine à l'aide de `metrocluster heal -phase root-aggregates` commande.
+
[listing]
----
mcc1A::> metrocluster heal -phase root-aggregates
[Job 137] Job succeeded: Heal Root Aggregates is successful
----
+
Si la guérison est vetotée, vous avez la possibilité de réémettre le `metrocluster heal` commande avec le paramètre -override-vetos. Si vous utilisez ce paramètre facultatif, le système remplace tout veto logiciel qui empêche l'opération de correction.

. Vérifier que l'opération de correction est terminée en utilisant le `metrocluster operation show` commande sur le cluster destination :
+
[listing]
----

mcc1A::> metrocluster operation show
  Operation: heal-root-aggregates
      State: successful
 Start Time: 7/29/2016 20:54:41
   End Time: 7/29/2016 20:54:42
     Errors: -
----
. Sur le module de contrôleur défaillant, débranchez les blocs d'alimentation.

